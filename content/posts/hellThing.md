+++
title = "What the hell is this thing?"
author = ["Dr Ian Hocking"]
date = 2023-04-08
lastmod = 2023-04-08T18:22:42+01:00
tags = ["AI", "ChatGPT", "GPT4"]
draft = false
+++

I've been looking through old drafts of my novel, **Flashback**, which features several technologies that did not exist when I wrote the first draft back in 2008 (or whenever it was). First, a time machine that used centrifuges to create a gravity-based mechanism for sending objects backward in time: fictional, and remains so. Second, a human-brain interface that imposed a donor personality on the brain into which the chip had been placed: still science fiction. Third, a credit-card sized personal assistant called Ego, which could listen for human speech, respond verbally, and take action (sometimes implicitly) to achieve the goals it had been set. In one scene, Ego is present when a hacker called Jem breaks into Saskia Brandt's locked study. It wants her to turn on the computer.

&gt; ‘What the hell is this thing? Some kind of PDA?’
&gt; The credit card was small, but its voice was rich. ‘I don’t understand. Please power-on the computer.’
&gt; ‘Where are you?’
&gt; ‘I don’t understand.’
&gt; Jem tapped the desktop computer’s power switch. She left the card next to it while she returned to the door and pressed her ear to the wood. Silence. ‘I want to know who is controlling this device.’
&gt; ‘I am.’
&gt; ‘I understand that. But where are you and who are you?’
&gt; ‘I am here and my name is Ego.’
&gt; Turning from the door, she paced the room like a gallery visitor perplexed by an exhibit. ‘Like the cat. Saskia’s cat is called Ego.’
&gt; A pause. ‘I didn’t know that.’
&gt; ‘What do you know?’
&gt; ‘Many things.’
&gt; Jem took the card and slapped it across the edge of the desk. ‘Don’t do that,’ it said.
&gt; ‘Tell me who you are, or I’ll cut the power to Saskia’s desktop.’
&gt; ‘My name is Ego. I am a computer-based agent.’

Things rarely come to pass in the manner they are described in science fiction, often because science fiction isn't really about predicting the future. But it was certainly at the back of my mind, as we approached the year 2023 (the 'future' in which several key scenes are set in Deja Vu and Flashback) that the work was about to become dated. The European Union, for instance, still included the UK. But the one thing I was quite confident about - and, indeed, would have bet could never happen - is that a conversation such as the one above could have taken place between a person and a computer. That, I think, is turning out to be wrong in interesting, amazing, and perhaps frightening ways.

I taught artificial intelligence to my psychology students for several years. The course was based on the Connectionist Summer School that used to run at Cambridge; I never attended the school itself, but was taught it second-hand by Jon Loose at the University of Exeter in the late 1990s. Those were quite heady days. Connectionism was visiting various areas of psychology with the subtlety of drive-by shootings. The flowchart, symbolic-first brigades became wary of these parsimonious, neuron-inspired models that often explained phenomena using simpler architectures, but the models were frustratingly fragile; start with the wrong number of nodes, the wrong bias, the wrong learning rate, and you had nothing. Get those parameters right and you had a model. But how did the behavior of these models map to people? For instance, if you had a recurrent network trained on language that could begin to pull out grammar merely through exposure (no Chomsky-like pre-programmed knowledge required), what empiricial predictions did that give us that we could test on humans in the lab? What, if any, claim did such a model make about time-related behaviours during language processing tasks like the skippig of the eye across words in a sentence, which we had long known to reflect processing difficulty?

Part of the reason I did my PhD in psycholinguistics is that I was intuitively certain of the ability for a well-motivated system to pull correspondences out of language on the basis of paying close attention to the data. The mechanism wasn't clear to me, but I had a hunch that prediction was the key. If you have to predict, you have to understand - maybe not completely - but you do have to understand. I don't want to present this as some great insight on my part, because it was a commonly held idea, even though it had vague, unpleasant associations with behaviorism, which had driven many of us into

Here we are more than twenty years later. Prediction indeed had the power. We just needed a way to practically implement it. (Steven Wolfram's 10k word blog post provides a nice overview: [What is ChatGPT and Why Does It Work?](<https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/>).)

GPT-4 has shown that this ostensibly simple task of attempting to predict the next element in a sequence (in this case, language) works. Simple prediction: in order to achieve it, it appears to have uncovered through statistical means some properties of the way in which humans understand reality, and internalized them. What knowledge does it contain? We're not sure, in detail. Is it self-aware? Almost certainly not. Is it intelligent? Yes, for generous definitions of intelligence. Is it coming for us? Maybe; we can't really tell at the moment. The immediate future is one that features a wonderful, naive, quick, and occasionally mendacious companion to most white-collar tasks. The longer-term future is less clear.

You can read about Microsoft researchers (otherwise unconnected to ChatGPT and GPT-4) finding [nascent abilities in GPT-4](<https://arxiv.org/abs/2303.12712>); somewhat credulous but full of examples where GPT-4 shows an uncanny ability to operate in domains that I would never have guessed would be accessible to a large language model. Lex Fridman talking to Eliezer Yudkowsky on [the Dangers of AI and the End of Human Civilization](<https://www.youtube.com/watch?v=AaTRHFaaPG8&t=4057s>).

What the hell is this thing? I'm not sure. It's quite possible that, fundamentally, we'll never know. The mind of GPT-4 is a matrix of fractional numbers that can give you advice on where to shop in Rome, in Italian, or discuss ancient Stoicism, or write the code to produce the Fibonacci sequence in any given computer language.
